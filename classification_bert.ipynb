{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Install Required Libraries"
      ],
      "metadata": {
        "id": "k1UEpT67PRyy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install transformers datasets torch scikit-learn"
      ],
      "metadata": {
        "id": "qy3_WNqMPQS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Prepare Your Dataset"
      ],
      "metadata": {
        "id": "KkPYEtG5PZe6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_csv('spam_data.csv')  # Your dataset with 'text' and 'label' columns\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n"
      ],
      "metadata": {
        "id": "Fb3JBm3QNpLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Load and Preprocess the Dataset"
      ],
      "metadata": {
        "id": "REJkMJA7Pifq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize the data\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], padding='max_length', truncation=True)\n",
        "\n",
        "train_df = train_df.to_dict(orient='records')\n",
        "test_df = test_df.to_dict(orient='records')\n",
        "\n",
        "train_encodings = tokenizer([x['text'] for x in train_df], truncation=True, padding=True, max_length=128)\n",
        "test_encodings = tokenizer([x['text'] for x in test_df], truncation=True, padding=True, max_length=128)\n",
        "\n",
        "# Convert labels to tensors\n",
        "import torch\n",
        "\n",
        "train_labels = torch.tensor([x['label'] for x in train_df])\n",
        "test_labels = torch.tensor([x['label'] for x in test_df])\n",
        "\n",
        "# Create TensorDatasets\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "train_dataset = TensorDataset(torch.tensor(train_encodings['input_ids']), torch.tensor(train_encodings['attention_mask']), train_labels)\n",
        "test_dataset = TensorDataset(torch.tensor(test_encodings['input_ids']), torch.tensor(test_encodings['attention_mask']), test_labels)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=16)\n",
        "test_dataloader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size=16)\n"
      ],
      "metadata": {
        "id": "Lje-GFL1NpOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Load and Fine-Tune the BERT Model"
      ],
      "metadata": {
        "id": "CnpPOELHPmUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForSequenceClassification, AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Load the pre-trained BERT model for sequence classification\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "\n",
        "# Set up the optimizer and learning rate scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
        "total_steps = len(train_dataloader) * 4  # Assuming 4 epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "lw9pfvF2NpSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Training Loop"
      ],
      "metadata": {
        "id": "LRWfh42dPtiE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import CrossEntropyLoss\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train_model(model, train_dataloader, optimizer, scheduler, num_epochs=4):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch + 1}/{num_epochs}')\n",
        "        for batch in tqdm(train_dataloader):\n",
        "            b_input_ids, b_input_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "        print(f\"Training loss: {loss.item()}\")\n",
        "\n",
        "train_model(model, train_dataloader, optimizer, scheduler)\n"
      ],
      "metadata": {
        "id": "_bVI2sUWNpVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Evaluate the Model"
      ],
      "metadata": {
        "id": "N6vwz9FCPzer"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "def evaluate_model(model, test_dataloader):\n",
        "    model.eval()\n",
        "    predictions, true_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_dataloader:\n",
        "            b_input_ids, b_input_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "            outputs = model(b_input_ids, attention_mask=b_input_mask)\n",
        "            logits = outputs.logits\n",
        "\n",
        "            predictions.append(logits.argmax(dim=-1).cpu().numpy())\n",
        "            true_labels.append(b_labels.cpu().numpy())\n",
        "\n",
        "    predictions = [item for sublist in predictions for item in sublist]\n",
        "    true_labels = [item for sublist in true_labels for item in sublist]\n",
        "\n",
        "    print(\"Accuracy:\", accuracy_score(true_labels, predictions))\n",
        "    print(\"Classification Report:\\n\", classification_report(true_labels, predictions))\n",
        "\n",
        "evaluate_model(model, test_dataloader)\n"
      ],
      "metadata": {
        "id": "8n99Yv51NpYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QFUIG8yBNpoq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}